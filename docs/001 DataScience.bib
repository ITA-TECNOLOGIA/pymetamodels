Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{EUCEN2018,
abstract = {Materials modelling-Terminology, classification and metadata This CEN Workshop Agreement has been drafted and approved by a Workshop of representatives of interested parties, the constitution of which is indicated in the foreword of this Workshop Agreement. The formal process followed by the Workshop in the development of this Workshop Agreement has been endorsed by the National Members of CEN but neither the National Members of CEN nor the CEN-CENELEC Management Centre can be held accountable for the technical content of this CEN Workshop Agreement or possible conflicts with standards or legislation. This CEN Workshop Agreement can in no way be held as being an official standard developed by CEN and its Members. This CEN Workshop Agreement is publicly available as a reference document from the CEN Members National Standard Bodies. CEN members are the national standards bodies},
author = {{European Committee for Standardization (CEN)}},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/European Committee for Standardization (CEN) - 2018 - CWA 172842018 Materials modelling - Terminology, classification and metadata.pdf:pdf},
number = {April},
title = {{CWA 17284:2018 Materials modelling - Terminology, classification and metadata}},
year = {2018}
}
@article{Weise2020,
abstract = {We present a novel Python package for the uncertainty and sensitivity analysis of computational models. The mathematical background is based on the non-intrusive generalized polynomial chaos method allowing one to treat the investigated models as black box systems, without interfering with their legacy code. Pygpc is optimized to analyze models with complex and possibly discontinuous transfer functions that are computationally costly to evaluate. The toolbox determines the uncertainty of multiple quantities of interest in parallel, given the uncertainties of the system parameters and inputs. It also yields gradient-based sensitivity measures and Sobol indices to reveal the relative importance of model parameters.},
author = {Weise, Konstantin and Po{\ss}ner, Lucas and M{\"{u}}ller, Erik and Gast, Richard and Kn{\"{o}}sche, Thomas R.},
doi = {10.1016/j.softx.2020.100450},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Weise et al. - 2020 - Pygpc A sensitivity and uncertainty analysis toolbox for Python.pdf:pdf},
issn = {23527110},
journal = {SoftwareX},
keywords = {Polynomial chaos,Sensitivity analysis,Uncertainty analysis},
pages = {100450},
publisher = {Elsevier B.V.},
title = {{Pygpc: A sensitivity and uncertainty analysis toolbox for Python}},
url = {https://doi.org/10.1016/j.softx.2020.100450},
volume = {11},
year = {2020}
}
@book{Mines2002,
author = {Mines, R.A.W},
booktitle = {International Journal of Impact Engineering},
doi = {10.1016/s0734-743x(02)00013-1},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Donald Berghaus (auth.) - Numerical Methods for Experimental Mechanics (2001, Springer US) - libgen.lc.pdf:pdf},
isbn = {9780792374039},
issn = {0734743X},
number = {9},
pages = {988},
title = {{Numerical methods for experimental mechanics}},
volume = {27},
year = {2002}
}
@article{Borgonovo2007,
abstract = {Uncertainty in parameters is present in many risk assessment problems and leads to uncertainty in model predictions. In this work, we introduce a global sensitivity indicator which looks at the influence of input uncertainty on the entire output distribution without reference to a specific moment of the output (moment independence) and which can be defined also in the presence of correlations among the parameters. We discuss its mathematical properties and highlight the differences between the present indicator, variance-based uncertainty importance measures and a moment independent sensitivity indicator previously introduced in the literature. Numerical results are discussed with application to the probabilistic risk assessment model on which Iman [A matrix-based approach to uncertainty and sensitivity analysis for fault trees. Risk Anal 1987;7(1):22-33] first introduced uncertainty importance measures. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Borgonovo, E.},
doi = {10.1016/j.ress.2006.04.015},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Borgonovo - 2007 - A new uncertainty importance measure.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Global sensitivity analysis,Importance measures,Probabilistic risk assessment,Uncertainty analysis,Uncertainty importance measures},
number = {6},
pages = {771--784},
title = {{A new uncertainty importance measure}},
volume = {92},
year = {2007}
}
@article{Campolongo2007,
abstract = {In 1991 Morris proposed an effective screening sensitivity measure to identify the few important factors in models with many factors. The method is based on computing for each input a number of incremental ratios, namely elementary effects, which are then averaged to assess the overall importance of the input. Despite its value, the method is still rarely used and instead local analyses varying one factor at a time around a baseline point are usually employed. In this piece of work we propose a revised version of the elementary effects method, improved in terms of both the definition of the measure and the sampling strategy. In the present form the method shares many of the positive qualities of the variance-based techniques, having the advantage of a lower computational cost, as demonstrated by the analytical examples. The method is employed to assess the sensitivity of a chemical reaction model for dimethylsulphide (DMS), a gas involved in climate change. Results of the sensitivity analysis open up the ground for model reconsideration: some model components may need a more thorough modelling effort while some others may need to be simplified. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Campolongo, Francesca and Cariboni, Jessica and Saltelli, Andrea},
doi = {10.1016/j.envsoft.2006.10.004},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Campolongo, Cariboni, Saltelli - 2007 - An effective screening design for sensitivity analysis of large models.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Dimethylsulphide (DMS),Effective sampling strategy,Model-free methods,Screening problem,Sensitivity analysis},
number = {10},
pages = {1509--1518},
title = {{An effective screening design for sensitivity analysis of large models}},
volume = {22},
year = {2007}
}
@article{MacKay1992,
abstract = {Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. “Occam's razor” is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.},
author = {MacKay, David J. C.},
doi = {10.1162/neco.1992.4.3.415},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/MacKay - 1992 - Bayesian Interpolation.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {3},
pages = {415--447},
title = {{Bayesian Interpolation}},
volume = {4},
year = {1992}
}
@article{Sidiropoulos2017,
abstract = {Tensors or multiway arrays are functions of three or more indices (i,j,k,⋯)-similar to matrices (two-way arrays), which are functions of two indices (r,c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.},
archivePrefix = {arXiv},
arxivId = {1607.01668},
author = {Sidiropoulos, Nicholas D. and {De Lathauwer}, Lieven and Fu, Xiao and Huang, Kejun and Papalexakis, Evangelos E. and Faloutsos, Christos},
doi = {10.1109/TSP.2017.2690524},
eprint = {1607.01668},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Sidiropoulos et al. - 2017 - Tensor Decomposition for Signal Processing and Machine Learning.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Cram{\'{e}}r-Rao bound,Gauss-Newton,NP-hard problems,Tensor decomposition,Tucker model,alternating direction method of multipliers,alternating optimization,canonical polyadic decomposition (CPD),classification,collaborative filtering,communications,gradient descent,harmonic retrieval,higher-order singular value decomposition (HOSVD),mixture modeling,multilinear singular value decomposition (MLSVD),parallel factor analysis (PARAFAC),rank,source separation,speech separation,stochastic gradient,subspace learning,tensor factorization,topic modeling,uniqueness},
number = {13},
pages = {3551--3582},
publisher = {IEEE},
title = {{Tensor Decomposition for Signal Processing and Machine Learning}},
volume = {65},
year = {2017}
}
@article{Iturbide2013,
abstract = {In this paper the LASSO and LARS estimators to fit auto-regressive time series models as well as OLS are compared. LASSO and LARS are two widely used methods to tackle the variable selection problem. To this end we used 4,004 different time series taken from the M1 and M3 time series competition. As expected, the experiments corroborates that LARS and LASSO derive models that outperform OLS models in terms of the mean square error. It is well known that LARS and LASSO behave similarly; however, the results obtained highlight their differences in terms of forecasting accuracy.},
author = {Iturbide, Eric and Cerda, Jaime and Graff, Mario},
doi = {10.1016/j.protcy.2013.04.035},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Iturbide, Cerda, Graff - 2013 - A Comparison between LARS and LASSO for Initialising the Time-Series Forecasting Auto-Regressive Equatio.pdf:pdf},
issn = {22120173},
journal = {Procedia Technology},
keywords = {ar,lars,lasso,models,n,ols,time series},
number = {443},
pages = {282--288},
title = {{A Comparison between LARS and LASSO for Initialising the Time-Series Forecasting Auto-Regressive Equations}},
volume = {7},
year = {2013}
}
@article{MacKay1994,
abstract = {The 1993 energy prediction competition involved the prediction of a series of building energy loads from a series of environmental input variables. Non-linear regression using`neuralusing`neural networks' is a popular technique for such modeling tasks. Since it is not obvious how large a time-window of inputs is appropriate, or what preprocessing of inputs is best, this can be viewed as a regression problem in which there are many possible input variables, some of which m a y actually be irrelevant to the prediction of the output variable. Because a anite data set will show random correlations between the irrelevant inputs and the output, any conventional neural network even with regularisation or`weightor`weight decay' will not set the coeecients for these junk inputs to zero. Thus the irrelevant v ariables will hurt the model's performance. The Automatic Relevance Determination ARD model puts a prior over the regression parameters which e m bodies the concept of relevance. This is done in a simple and`softand`soft' way b y i n troducing multiple regularisation constants, one associated with each input. Using Bayesian methods, the reg-ularisation constants for junk inputs are automatically inferred to be large, preventing those inputs from causing signiicant o verrtting. An entry using the ARD model won the competition by a signiicant margin. 1 Overview of Bayesian modeling methods A practical Bayesian framework for adaptive data modeling has been described in MacKay 1992. In this framework, the overall aim is to develop probabilistic models that are well matched to the data, and make optimal predictions with those models. Neural network learning, for example, is interpreted as an inference of the most probable parameters for a model, given the training data. The search in model space i.e., the space of architec-tures, noise models, preprocessings, regularizers and regularisation constants can then also be treated as an inference problem, where we infer the relative probability of alternative models, given the data. Bayesian model comparison naturally embodies Occam's razor, the principle that states a preference for simple models. Bayesian optimization of model control parameters has four important advantages. 1 No validation set is needed; so all the training data can be devoted to both model ltting and model comparison. 2 Regularisation constants can be optimized on-line, i.e. simultaneously with the optimization of ordinary model parameters. 3 The Bayesian objective function is not noisy, as a cross-validation measure is. 4 Because the gradient of the evidence with respect to the control parameters can be evaluated, it is possible to optimism a large number of control parameters simultaneously. Bayesian inference for neural nets can be implemented numerically by a deterministic method involving Gaussian approximations, th{\`{e}}evidence' framework MacKay 1992, or},
author = {MacKay, D.J.C.},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/MacKay - 1994 - Bayesian non-linear modelling for the energy prediction competition.pdf:pdf},
journal = {ASHRAE Transcations},
pages = {448--472},
title = {{Bayesian non-linear modelling for the energy prediction competition}},
url = {https://bayes.wustl.edu/MacKay/pred.pdf},
volume = {4},
year = {1994}
}
@article{Plischke2013,
abstract = {Simulation models support managers in the solution of complex problems. International agencies recommend uncertainty and global sensitivity methods as best practice in the audit, validation and application of scientific codes. However, numerical complexity, especially in the presence of a high number of factors, induces analysts to employ less informative but numerically cheaper methods. This work introduces a design for estimating global sensitivity indices from given data (including simulation input-output data), at the minimum computational cost. We address the problem starting with a statistic based on the L1-norm. A formal definition of the estimators is provided and corresponding consistency theorems are proved. The determination of confidence intervals through a bias-reducing bootstrap estimator is investigated. The strategy is applied in the identification of the key drivers of uncertainty for the complex computer code developed at the National Aeronautics and Space Administration (NASA) assessing the risk of lunar space missions. We also introduce a symmetry result that enables the estimation of global sensitivity measures to datasets produced outside a conventional input-output functional framework. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Plischke, Elmar and Borgonovo, Emanuele and Smith, Curtis L.},
doi = {10.1016/j.ejor.2012.11.047},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Plischke, Borgonovo, Smith - 2013 - Global sensitivity measures from given data.pdf:pdf},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Global sensitivity analysis,Simulation,Uncertainty analysis},
number = {3},
pages = {536--550},
publisher = {Elsevier B.V.},
title = {{Global sensitivity measures from given data}},
url = {http://dx.doi.org/10.1016/j.ejor.2012.11.047},
volume = {226},
year = {2013}
}
@techreport{Lahuerta2021,
address = {Zaragoza},
author = {Lahuerta, F},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Lahuerta - 2021 - An{\'{a}}lisis de las capacidades de Ansys- Optislang orientadas a dise{\~{n}}o generativo y o ptimizaci{\'{o}}n 2300_I219011.pdf:pdf},
institution = {Itainnova},
title = {{An{\'{a}}lisis de las capacidades de Ansys- Optislang orientadas a dise{\~{n}}o generativo y o ptimizaci{\'{o}}n [2300_I219011]}},
year = {2021}
}
@article{Song2021,
abstract = {Global sensitivity analysis (GSA) is a useful tool to evaluate the influence of input variables in the whole distribution range. Variance-based methods and moment-independent methods are widely studied and popular GSA techniques despite their several shortcomings. Since probability weighted moments (PWMs) include more information than classical moments and can be accurately estimated from small samples, a novel global sensitivity measure based on PWMs is proposed. Then, two methods are introduced to estimate the proposed measure, i.e., double-loop-repeated-set numerical estimation and double-loop-single-set numerical estimation. Several numerical and engineering examples are used to show its advantages.},
author = {Song, Shufang and Wang, Lu},
doi = {10.3390/sym13010090},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Song, Wang - 2021 - A Novel Global Sensitivity Measure Based on Probability Weighted Moments.pdf:pdf},
journal = {Symmetry},
keywords = {global sensitivity analysis,global sensitivity analysis (GSA),gsa,importance measure,moment-independent sensitivity analysis,probability weighted moment,probability weighted moment (PWM),pwm,variance-based sensitivity analysis},
number = {90},
title = {{A Novel Global Sensitivity Measure Based on Probability Weighted Moments}},
volume = {13},
year = {2021}
}
@article{Brownlee2019,
abstract = {Generative Adversarial Networks are a type of deep learning generative model that can achieve startlingly photorealistic results on a range of image synthesis and image-to-image translation problems. In this new Ebook written in the friendly Machine Learning Mastery style that you're used to, skip the math and jump straight to getting results. With clear explanations, standard Python libraries, and step-by-step tutorial lessons, you'll discover how to develop Generative Adversarial Networks for your own computer vision projects.},
author = {Brownlee, Jason},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Brownlee - 2019 - Generative Adversarial Networks with Python.pdf:pdf},
journal = {Machine Learning Mastery},
pages = {1--654},
title = {{Generative Adversarial Networks with Python}},
url = {https://books.google.com.sa/books/about/Generative_Adversarial_Networks_with_Pyt.html?id=YBimDwAAQBAJ&redir_esc=y%0Ahttps://machinelearningmastery.com/generative_adversarial_networks/},
year = {2019}
}
@article{Luo2018,
abstract = {DeepWarp is an efficient and highly re-usable deep neural network (DNN) based nonlinear deformable simulation framework. Unlike other deep learning applications such as image recognition, where different inputs have a uniform and consistent format (e.g. an array of all the pixels in an image), the input for deformable simulation is quite variable, high-dimensional, and parametrization-unfriendly. Consequently, even though DNN is known for its rich expressivity of nonlinear functions, directly using DNN to reconstruct the force-displacement relation for general deformable simulation is nearly impossible. DeepWarp obviates this difficulty by partially restoring the force-displacement relation via warping the nodal displacement simulated using a simplistic constitutive model -- the linear elasticity. In other words, DeepWarp yields an incremental displacement fix based on a simplified (therefore incorrect) simulation result other than returning the unknown displacement directly. We contrive a compact yet effective feature vector including geodesic, potential and digression to sort training pairs of per-node linear and nonlinear displacement. DeepWarp is robust under different model shapes and tessellations. With the assistance of deformation substructuring, one DNN training is able to handle a wide range of 3D models of various geometries including most examples shown in the paper. Thanks to the linear elasticity and its constant system matrix, the underlying simulator only needs to perform one pre-factorized matrix solve at each time step, and DeepWarp is able to simulate large models in real time.},
archivePrefix = {arXiv},
arxivId = {1803.09109},
author = {Luo, Ran and Shao, Tianjia and Wang, Huamin and Xu, Weiwei and Zhou, Kun and Yang, Yin},
doi = {10.1109/TVCG.2018.2881451},
eprint = {1803.09109},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Luo et al. - 2018 - DeepWarp DNN-based Nonlinear Deformation.pdf:pdf},
pages = {1--13},
title = {{DeepWarp: DNN-based Nonlinear Deformation}},
url = {http://arxiv.org/abs/1803.09109%0Ahttp://dx.doi.org/10.1109/TVCG.2018.2881451},
year = {2018}
}
@book{Bonnans2003,
author = {Bonnans, J. Frederic and Gilbert, J. Charles and Lemarechal, C and Sagastizabal, Claudia},
booktitle = {Numerical Optimization},
doi = {10.1007/978-3-540-35447-5},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/[Universitext ] J. Fr{\'{e}}d{\'{e}}ric Bonnans, J. Charles Gilbert, Claude Lemar{\'{e}}chal, Clau - Numerical Optimization_ Theoretical and Practical Asp.pdf:pdf},
isbn = {978-3-540-35445-1},
publisher = {Springer},
title = {{Numerical Optimization}},
year = {2003}
}
@book{Lee,
abstract = {As an experienced JavaScript developer moving to server-side programming, you need to implement classic data structures and algorithms associated with conventional object-oriented languages like C# and Java. This practical guide shows you how to work hands-on with a variety of storage mechanisms—including linked lists, stacks, queues, and graphs—within the constraints of the JavaScript environment. Determine which data structures and algorithms are most appropriate for the problems you're trying to solve, and understand the tradeoffs when using them in a JavaScript program.},
author = {Lee, Kent D},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Data Structures & Algorithms With Python.pdf:pdf},
isbn = {9783319130712},
title = {{Data Structures and Algorithms with Python}}
}
@article{Saltelli2010,
abstract = {Variance based methods have assessed themselves as versatile and effective among the various available techniques for sensitivity analysis of model output. Practitioners can in principle describe the sensitivity pattern of a model Y = f (X1, X2, ..., Xk) with k uncertain input factors via a full decomposition of the variance V of Y into terms depending on the factors and their interactions. More often practitioners are satisfied with computing just k first order effects and k total effects, the latter describing synthetically interactions among input factors. In sensitivity analysis a key concern is the computational cost of the analysis, defined in terms of number of evaluations of f (X1, X2, ..., Xk) needed to complete the analysis, as f (X1, X2, ..., Xk) is often in the form of a numerical model which may take long processing time. While the computational cost is relatively cheap and weakly dependent on k for estimating first order effects, it remains expensive and strictly k-dependent for total effect indices. In the present note we compare existing and new practices for this index and offer recommendations on which to use. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Saltelli, Andrea and Annoni, Paola and Azzini, Ivano and Campolongo, Francesca and Ratto, Marco and Tarantola, Stefano},
doi = {10.1016/j.cpc.2009.09.018},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Saltelli et al. - 2010 - Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index.pdf:pdf},
issn = {00104655},
journal = {Computer Physics Communications},
number = {2},
pages = {259--270},
publisher = {Elsevier B.V.},
title = {{Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index}},
url = {http://dx.doi.org/10.1016/j.cpc.2009.09.018},
volume = {181},
year = {2010}
}
@article{Gao2012a,
abstract = {In this paper, we first prove that the expansion and contraction steps of the Nelder-Mead simplex algorithm possess a descent property when the objective function is uniformly convex. This property provides some new insights on why the standard Nelder-Mead algorithm becomes inefficient in high dimensions. We then propose an implementation of the Nelder-Mead method in which the expansion, contraction, and shrink parameters depend on the dimension of the optimization problem. Our numerical experiments show that the new implementation outperforms the standard Nelder-Mead method for high dimensional problems. {\textcopyright} Springer Science+Business Media, LLC 2010.},
author = {Gao, Fuchang and Han, Lixing},
doi = {10.1007/s10589-010-9329-3},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Implementing_the_Nelder-Mead_simplex_algorithm_wit.pdf:pdf},
isbn = {1058901093293},
issn = {09266003},
journal = {Computational Optimization and Applications},
keywords = {Adaptive parameter,Nelder-Mead method,Optimization,Polytope,Simplex},
number = {1},
pages = {259--277},
title = {{Implementing the Nelder-Mead simplex algorithm with adaptive parameters}},
volume = {51},
year = {2012}
}
@article{Brownlee2017,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little $q$-Jacobi polynomials in the limit $q=-1$. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for $q=-1$.},
author = {Brownlee, Jason},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Brownlee - 2017 - 00 ML Mastery - Understand You Data, Create Accurate Models and Work Projects End-to-End.pdf:pdf},
journal = {Machine Learning Mastery},
pages = {399--404},
title = {{00 ML Mastery - Understand You Data, Create Accurate Models and Work Projects End-to-End}},
volume = {91},
year = {2017}
}
@article{Sobol2010,
abstract = {We introduce new global sensitivity measures called derivative based global sensitivity measures (DGSM). We also show that there is a link between DGSM and Sobol' total sensitivity indices which makes this approach theoretically sound and general. It can be seen as the generalization of the Morris method. The computational time required for numerical evaluation of DGSM can be much lower than that for estimation of the Sobol' sensitivity indices although it is problem dependent. The efficiency of the method can be further improved by using the automatic differentiation algorithm for calculation DGSM.},
author = {Sobol, I. M. and Kucherenko, S.},
doi = {10.1016/j.sbspro.2010.05.208},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Sobol', Kucherenko - 2010 - Derivative based global sensitivity measures.pdf:pdf},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Derivative based global sensitivity measure,Global sensitivity analysis,Global sensitivity index,Monte Carlo method},
number = {6},
pages = {7745--7746},
publisher = {Elsevier Masson SAS},
title = {{Derivative based global sensitivity measures}},
url = {http://dx.doi.org/10.1016/j.sbspro.2010.05.208},
volume = {2},
year = {2010}
}
@article{Endres2018,
abstract = {The simplicial homology global optimisation (SHGO) algorithm is a general purpose global optimisation algorithm based on applications of simplicial integral homology and combinatorial topology. SHGO approximates the homology groups of a complex built on a hypersurface homeomorphic to a complex on the objective function. This provides both approximations of locally convex subdomains in the search space through Sperner's lemma and a useful visual tool for characterising and efficiently solving higher dimensional black and grey box optimisation problems. This complex is built up using sampling points within the feasible search space as vertices. The algorithm is specialised in finding all the local minima of an objective function with expensive function evaluations efficiently which is especially suitable to applications such as energy landscape exploration. SHGO was initially developed as an improvement on the topographical global optimisation (TGO) method. It is proven that the SHGO algorithm will always outperform TGO on function evaluations if the objective function is Lipschitz smooth. In this paper SHGO is applied to non-convex problems with linear and box constraints with bounds placed on the variables. Numerical experiments on linearly constrained test problems show that SHGO gives competitive results compared to TGO and the recently developed Lc-DISIMPL algorithm as well as the PSwarm, LGO and DIRECT-L1 algorithms. Furthermore SHGO is compared with the TGO, basinhopping (BH) and differential evolution (DE) global optimisation algorithms over a large selection of black-box problems with bounds placed on the variables from the SciPy benchmarking test suite. A Python implementation of the SHGO and TGO algorithms published under a MIT license can be found from https://bitbucket.org/upiamcompthermo/shgo/.},
author = {Endres, Stefan C. and Sandrock, Carl and Focke, Walter W.},
doi = {10.1007/s10898-018-0645-y},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Endres2018shgo_jogo.pdf:pdf},
issn = {15732916},
journal = {Journal of Global Optimization},
keywords = {Computational homology,Global optimisation,SHGO},
number = {2},
pages = {181--217},
title = {{A simplicial homology algorithm for Lipschitz optimisation}},
volume = {72},
year = {2018}
}
@article{Agarwal2019,
abstract = {A major bottleneck of the current Machine Learning (ML) workflow is the time consuming, error prone engineering required to get data from a datastore or a database (DB) to the point an ML algorithm can be applied to it. Hence, we explore the feasibility of directly integrating prediction functionality on top of a data store or DB. Such a system ideally: (i) provides an intuitive prediction query interface which alleviates the unwieldy data engineering; (ii) provides state-of-the-art statistical accuracy while ensuring incremental model update, low model training time and low latency for making predictions. As the main contribution we explicitly instantiate a proof-of-concept, tspDB, which directly integrates with PostgreSQL. We rigorously test tspDB's statistical and computational performance against the state-of-the-art time series algorithms, including a Long-Short-Term-Memory (LSTM) neural network and DeepAR (industry standard deep learning library by Amazon). Statistically, on standard time series benchmarks, tspDB outperforms LSTM and DeepAR with 1.1-1.3x higher relative accuracy. Computationally, tspDB is 59-62x and 94-95x faster compared to LSTM and DeepAR in terms of median ML model training time and prediction query latency, respectively. Further, compared to PostgreSQL's bulk insert time and its SELECT query latency, tspDB is slower only by 1.3x and 2.6x respectively. That is, tspDB is a real-time prediction system in that its model training / prediction query time is similar to just inserting / reading data from a DB. As an algorithmic contribution, we introduce an incremental multivariate matrix factorization based time series method, which tspDB is built off. We show this method also allows one to produce reliable prediction intervals by accurately estimating the time-varying variance of a time series, thereby addressing an important problem in time series analysis.},
archivePrefix = {arXiv},
arxivId = {1903.07097},
author = {Agarwal, Anish and Alomar, Abdullah and Shah, Devavrat},
eprint = {1903.07097},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Agarwal, Alomar, Shah - 2019 - tspDB Time Series Predict DB.pdf:pdf},
pages = {1--31},
title = {{tspDB: Time Series Predict DB}},
url = {http://arxiv.org/abs/1903.07097},
year = {2019}
}
@article{Pedregosa2019,
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {{Fabian Pedregosa}, Ga{\"{e}}l and Varoquaux, Alexandre and Gramfort, Vincent Michel and {Bertrand Thirion}, Olivier Grisel and {Mathieu Blondel}, Peter and Prettenhofer, Ron Weiss and {Vincent Dubourg}, Jake and Vanderplas, Alexandre Passos and {David Cournapeau}, Matthieu and Brucher, Matthieu Perrot and Duchesnay, {\'{E}}douard},
doi = {10.48550/arXiv.1201.0490},
eprint = {1201.0490},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/pedregosa11a.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
volume = {12},
year = {2011}
}
@article{Zarnoch2009,
author = {Zarnoch, Stanley J},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Zarnoch - 2009 - Testing hypotheses for differences between linear regression lines.pdf:pdf},
keywords = {as for example when,contrasts,dummy variables,f-test,for differences between means,glm,intercept,is often used to,of conditional error,sas,slope,test,the,the general linear model},
title = {{Testing hypotheses for differences between linear regression lines}},
year = {2009}
}
@book{Christensen2009,
abstract = {Modern Evolutionary Economics Evolutionary economics sees the economy as always in motion with change being driven largely by continuing innovation. This approach to economics, heavily infl uenced by the work of Joseph Schumpeter, saw a revival as an alternative way of thinking about economic advancement as a result of Richard Nelson and Sidney Winter's seminal book, An Evolutionary Theory of Economic Change , fi rst published in 1982. In this long- awaited follow- up, Nelson is joined by leading fi gures in the fi eld of evolutionary economics, reviewing in detail how this perspective has been manifest in various areas of economic inquiry where evolutionary economists have been active. Providing the perfect overview for interested economists and social scientists, readers will learn how in each of the diverse fi elds featured, evolutionary economics has enabled an improved understanding of how and why economic progress occurs. Richard},
author = {Christensen, Peter W. and Klarbring, Anders},
booktitle = {Springer},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Christensen, Klarbring - 2009 - An Introduction to Structural Optimization.pdf:pdf},
isbn = {978-1-4020-8665-6},
pages = {214},
publisher = {Springer-Verlag},
title = {{An Introduction to Structural Optimization}},
year = {2009}
}
@article{Nguyen2015,
abstract = {The choice of sensitivity analysis methods for a model often relies on the behavior of model outputs. However, many building energy models are “black-box” functions whose behavior of simulated results is usually unknown or uncertain. This situation raises a question of how to correctly choose a sensitivity analysis method and its settings for building simulation. A performance comparison of nine sensitivity analysis methods has been carried out by means of computational experiments and building energy simulation. A comprehensive test procedure using three benchmark functions and two real-world building energy models was proposed. The degree of complexity was gradually increased by carefully-chosen test problems. Performance of these methods was compared through the ranking of variables' importance, variables' sensitivity indices, interaction among variables, and computational cost for each method. Test results show the consistency between the Fourier Amplitude Sensitivity Test (FAST) and the Sobol method. Some evidences found from the tests indicate that performance of other methods was unstable, especially with the non-monotonic test problems.},
author = {Nguyen, Anh Tuan and Reiter, Sigrid},
doi = {10.1007/s12273-015-0245-4},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Nguyen, Reiter - 2015 - A performance comparison of sensitivity analysis methods for building energy models.pdf:pdf},
issn = {19968744},
journal = {Building Simulation},
keywords = {Monte Carlo approach,Morris method,comparison,regression-based sensitivity analysis,variance-based sensitivity analysis},
number = {6},
pages = {651--664},
title = {{A performance comparison of sensitivity analysis methods for building energy models}},
volume = {8},
year = {2015}
}
@article{M.1814,
author = {Hamby, D M},
doi = {10.1097/00004032-199502000-00005},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Hamby - 1995 - A comparison of sensitivity analysis techniques.pdf:pdf},
issn = {17581605},
journal = {Health Physics Society},
keywords = {dosimetry,environ-,modeling,statistics,tritium},
number = {2},
pages = {195--204},
title = {{A comparison of sensitivity analysis techniques}},
volume = {62},
year = {1995}
}
@article{Storn1997,
abstract = {A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
author = {Storn, Rainer and Price, Kenneth},
doi = {10.1023/A:1008202821328},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/TR-95-012 (1).pdf:pdf},
issn = {1573-2916},
journal = {Journal of Global Optimization},
number = {4},
pages = {341--359},
title = {{Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces}},
url = {https://doi.org/10.1023/A:1008202821328},
volume = {11},
year = {1997}
}
@article{Wang2015,
abstract = {We present a data-driven method for deformation capture and modeling of general soft objects. We adopt an iterative framework that consists of one component for physics-based deformation tracking and another for spacetime optimization of deformation parameters. Low cost depth sensors are used for the deformation capture, and we do not require any force-displacement measurements, thus making the data capture a cheap and convenient process. We augment a state-of-the-art probabilistic tracking method to robustly handle noise, occlusions, fast movements and large deformations. The spacetime optimization aims to match the simulated trajectories with the tracked ones. The optimized deformation model is then used to boost the accuracy of the tracking results, which can in turn improve the deformation parameter estimation itself in later iterations. Numerical experiments demonstrate that the tracking and parameter optimization components complement each other nicely. Our spacetime optimization of the deformation model includes not only the material elasticity parameters and dynamic damping coefficients, but also the reference shape which can differ significantly from the static shape for soft objects. The resulting optimization problem is highly nonlinear in high dimensions, and challenging to solve with previous methods. We propose a novel splitting algorithm that alternates between reference shape optimization and deformation parameter estimation, and thus enables tailoring the optimization of each subproblem more efficiently and robustly. Our system enables realistic motion reconstruction as well as synthesis of virtual soft objects in response to user stimulation. Validation experiments show that our method not only is accurate, but also compares favorably to existing techniques. We also showcase the ability of our system with high quality animations generated from optimized deformation parameters for a variety of soft objects, such as live plants and fabricated models.},
author = {Wang, Bin and Wu, Longhua and Yin, Kang Kang and Ascher, Uri and Liu, Libin and Huang, Hui},
doi = {10.1145/2766911},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Wang et al. - 2015 - Deformation capture and modeling of soft objects.pdf:pdf},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Deformation modeling,FEM,Motion capture},
number = {4},
title = {{Deformation capture and modeling of soft objects}},
volume = {34},
year = {2015}
}
@article{Herman2017,
author = {Herman, J. and Usher, W.},
doi = {10.21105/joss.00097},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Herman, Usher - 2017 - SALib An open-source Python library for sensitivity analysis.pdf:pdf},
journal = {Journal of Open Source Software},
number = {9},
title = {{SALib: An open-source Python library for sensitivity analysis}},
volume = {2},
year = {2017}
}
@article{Brownlee2020,
abstract = {Probability is foundational to machine learning and required background for machine learning practitioners. ? Probability is a prerequisite in most courses and books on applied machine learning. ? Probability methods are used at each step in an applied machine learning project. ? Probabilistic frameworks underlie the training of many machine learning algorithms. Data distributions and statistics don't make sense without probability. Fitting models on a training dataset does not make sense without probability. Interpreting the performance of a stochastic learning algorithm does not make sense without probability. A machine learning practitioner cannot be effective without an understanding and appreciation of the basic concepts and methods from the field of probability. Practitioners},
author = {Brownlee, Jason},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Brownlee - 2020 - Probability for Machine Learning Discover how to harness uncertainty with Python.pdf:pdf},
journal = {Machine Learning Mastery},
pages = {319},
title = {{Probability for Machine Learning : Discover how to harness uncertainty with Python}},
year = {2020}
}
@article{Virtanen2020,
abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
archivePrefix = {arXiv},
arxivId = {1907.10121},
author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, St{\'{e}}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R.J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^{o}}nio H. and Pedregosa, Fabian and van Mulbregt, Paul and Vijaykumar, Aditya and Bardelli, Alessandro Pietro and Rothberg, Alex and Hilboll, Andreas and Kloeckner, Andreas and Scopatz, Anthony and Lee, Antony and Rokem, Ariel and Woods, C. Nathan and Fulton, Chad and Masson, Charles and H{\"{a}}ggstr{\"{o}}m, Christian and Fitzgerald, Clark and Nicholson, David A. and Hagen, David R. and Pasechnik, Dmitrii V. and Olivetti, Emanuele and Martin, Eric and Wieser, Eric and Silva, Fabrice and Lenders, Felix and Wilhelm, Florian and Young, G. and Price, Gavin A. and Ingold, Gert Ludwig and Allen, Gregory E. and Lee, Gregory R. and Audren, Herv{\'{e}} and Probst, Irvin and Dietrich, J{\"{o}}rg P. and Silterra, Jacob and Webber, James T. and Slavi{\v{c}}, Janko and Nothman, Joel and Buchner, Johannes and Kulick, Johannes and Sch{\"{o}}nberger, Johannes L. and {de Miranda Cardoso}, Jos{\'{e}} Vin{\'{i}}cius and Reimer, Joscha and Harrington, Joseph and Rodr{\'{i}}guez, Juan Luis Cano and Nunez-Iglesias, Juan and Kuczynski, Justin and Tritz, Kevin and Thoma, Martin and Newville, Matthew and K{\"{u}}mmerer, Matthias and Bolingbroke, Maximilian and Tartre, Michael and Pak, Mikhail and Smith, Nathaniel J. and Nowaczyk, Nikolai and Shebanov, Nikolay and Pavlyk, Oleksandr and Brodtkorb, Per A. and Lee, Perry and McGibbon, Robert T. and Feldbauer, Roman and Lewis, Sam and Tygier, Sam and Sievert, Scott and Vigna, Sebastiano and Peterson, Stefan and More, Surhud and Pudlik, Tadeusz and Oshima, Takuya and Pingel, Thomas J. and Robitaille, Thomas P. and Spura, Thomas and Jones, Thouis R. and Cera, Tim and Leslie, Tim and Zito, Tiziano and Krauss, Tom and Upadhyay, Utkarsh and Halchenko, Yaroslav O. and V{\'{a}}zquez-Baeza, Yoshiki},
doi = {10.1038/s41592-019-0686-2},
eprint = {1907.10121},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/s41592-019-0686-2.pdf:pdf},
issn = {15487105},
journal = {Nature Methods},
number = {3},
pages = {261--272},
pmid = {32015543},
title = {{SciPy 1.0: fundamental algorithms for scientific computing in Python}},
volume = {17},
year = {2020}
}
@article{Saltelli2002,
abstract = {This paper deals with computations of sensitivity indices in sensitivity analysis. Given a mathematical or computational model y = f(x1,x2,...,xk), where the input factors xi's are uncorrelated with one another, one can see y as the realization of a stochastic process obtained by sampling each of the xi from its marginal distribution. The sensitivity indices are related to the decomposition of the variance of y into terms either due to each xi taken singularly (first order indices), as well as into terms due to the cooperative effects of more than one xi. In this paper we assume that one has computed the full set of first order sensitivity indices as well as the full set of total-order sensitivity indices (a fairly common strategy in sensitivity analysis), and show that in this case the same set of model evaluations can be used to compute double estimates of: the total effect of two factors taken together, for all such (2k) couples, where k is the dimensionality of the model; the total effect of k-2 factors taken together, for all (2k) such (k-2) ples. We further introduce a new strategy for the computation of the full sets of first plus total order sensitivity indices that is about 50% cheaper in terms of model evaluations with respect to previously published works. We discuss separately the case where the input factors xi's are not independent from each other. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Saltelli, Andrea},
doi = {10.1016/S0010-4655(02)00280-1},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Saltelli - 2002 - Making best use of model evaluations to compute sensitivity indices.pdf:pdf},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Importance measures,Sensitivity analysis,Sensitivity indices,Sensitivity measures},
number = {2},
pages = {280--297},
title = {{Making best use of model evaluations to compute sensitivity indices}},
volume = {145},
year = {2002}
}
@article{Priem2020,
abstract = {Bayesian optimization methods have been successfully applied to black box optimization problems that are expensive to evaluate. In this paper, we adapt the so-called super efficient global optimization algorithm to solve more accurately mixed constrained problems. The proposed approach handles constraints by means of upper trust bound, the latter encourages exploration of the feasible domain by combining the mean prediction and the associated uncertainty function given by the Gaussian processes. On top of that, a refinement procedure, based on a learning rate criterion, is introduced to enhance the exploitation and exploration trade-off. We show the good potential of the approach on a set of numerical experiments. Finally, we present an application to conceptual aircraft configuration upon which we show the superiority of the proposed approach compared to a set of the state-of-the-art black box optimization solvers.},
archivePrefix = {arXiv},
arxivId = {2005.05067},
author = {Priem, R. and Bartoli, N. and Diouane, Y. and Sgueglia, A.},
doi = {10.1016/j.ast.2020.105980},
eprint = {2005.05067},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/2005.05067.pdf:pdf},
issn = {12709638},
journal = {Aerospace Science and Technology},
keywords = {Bayesian optimization,Black box optimization,Gaussian process,Global optimization,Mixed constrained optimization},
number = {1},
title = {{Upper trust bound feasibility criterion for mixed constrained Bayesian optimization with application to aircraft design}},
volume = {105},
year = {2020}
}
@article{Campolongo2011a,
abstract = {The present work is a sequel to a recent one published on this journal where the superiority of 'radial design' to compute the 'total sensitivity index' was ascertained. Both concepts belong to sensitivity analysis of model output. A radial design is the one whereby starting from a random point in the hyperspace of the input factors one step in turn is taken for each factor. The procedure is iterated a number of times with a different starting random point as to collect a sample of elementary shifts for each factor. The total sensitivity index is a powerful sensitivity measure which can be estimated based on such a sample. Given the similarity between the total sensitivity index and a screening test known as method of the elementary effects (or method of Morris), we test the radial design on this method. Both methods are best practices: the total sensitivity index in the class of the quantitative measures and the elementary effects in that of the screening methods. We find that the radial design is indeed superior even for the computation of the elementary effects method. This opens the door to a sensitivity analysis strategy whereby the analyst can start with a small number of points (screening-wise) and then - depending on the results - possibly increase the numeral of points up to compute a fully quantitative measure. Also of interest to practitioners is that a radial design is nothing else than an iterated 'One factor At a Time' (OAT) approach. OAT is a radial design of size one. While OAT is not a good practice, modelers in all domains keep using it for sensitivity analysis for reasons discussed elsewhere (Saltelli and Annoni, 2010) [23]. With the present approach modelers are offered a straightforward and economic upgrade of their OAT which maintain OAT's appeal of having just one factor moved at each step. {\textcopyright} 2011 Published by Elsevier B.V. All rights reserved.},
author = {Campolongo, Francesca and Saltelli, Andrea and Cariboni, Jessica},
doi = {10.1016/j.cpc.2010.12.039},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Campolongo, Saltelli, Cariboni - 2011 - From screening to quantitative sensitivity analysis. A unified approach.pdf:pdf},
issn = {00104655},
journal = {Computer Physics Communications},
keywords = {Elementary effect method,Radial design,Sensitivity analysis,Variance based measures},
number = {4},
pages = {978--988},
publisher = {Elsevier B.V.},
title = {{From screening to quantitative sensitivity analysis. A unified approach}},
url = {http://dx.doi.org/10.1016/j.cpc.2010.12.039},
volume = {182},
year = {2011}
}
@article{Sobol2009,
abstract = {A model function f(x1,...,xn) defined in the unit hypercube Hn with Lebesque measure dx = dx1...dxn is considered. If the function is square integrable, global sensitivity indices provide adequate estimates for the influence of individual factors xi or groups of such factors. Alternative estimators that require less computer time can also be used. If the function f is differentiable, functionals depending on ∂f/∂xi have been suggested as estimators for the influence of xi. The Morris importance measure modified by Campolongo, Cariboni and Saltelli $\mu$* is an approximation of the functional $\mu$i = ∫Hn fenced(∂ f / ∂ xi) d x. In this paper a similar functional is studied$\nu$i = ∫Hn fenced(frac(∂ f, ∂ xi))2 d xEvidently, $\mu$i ≤ sqrt($\nu$i), and $\nu$i ≤ C $\mu$i if fenced(∂ f / ∂ xi) ≤ C. A link between $\nu$i and the sensitivity index Sit o t is established:Sit o t ≤ frac($\nu$i, $\pi$2 D)where D is the total variance of f(x1,...,xn). Thus small $\nu$i imply small Sit o t, and unessential factors xi (that is xi corresponding to a very small Sit o t) can be detected analyzing computed values $\nu$1,...,$\nu$n. However, ranking influential factors xi using these values can give false conclusions. Generalized Sit o t and $\nu$i can be applied in situations where the factors x1,...,xn are independent random variables. If xi is a normal random variable with variance $\sigma$i2, then Sit o t ≤ $\nu$i $\sigma$i2 / D. {\textcopyright} 2009 IMACS.},
author = {Sobol', I. M. and Kucherenko, S.},
doi = {10.1016/j.matcom.2009.01.023},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Sobol', Kucherenko - 2009 - Derivative based global sensitivity measures and their link with global sensitivity indices.pdf:pdf},
issn = {03784754},
journal = {Mathematics and Computers in Simulation},
keywords = {Derivative based global sensitivity measure,Global sensitivity index,Morris method,Quasi Monte Carlo method},
number = {10},
pages = {3009--3017},
title = {{Derivative based global sensitivity measures and their link with global sensitivity indices}},
volume = {79},
year = {2009}
}
@article{Iman1981,
abstract = {This is the first part of a two-part article presenting a statistical approach to the sensitivity analysis of computer models. Part I defines the objectives of sensitivity analysis and presents a c...},
author = {Iman, Ronald L. and Helton, Jon C. and Campbell, James E.},
doi = {10.1080/00224065.1981.11978748},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Iman, Helton, Campbell - 1981 - An Approach to Sensitivity Analysis of Computer Models Part I—Introduction, Input Variable Selection and.pdf:pdf},
issn = {0022-4065},
journal = {Journal of Quality Technology},
number = {3},
pages = {174--183},
title = {{An Approach to Sensitivity Analysis of Computer Models: Part I—Introduction, Input Variable Selection and Preliminary Variable Assessment}},
volume = {13},
year = {1981}
}
@misc{pyansys2021,
publisher = {Ansys Python development organization},
title = {{PyAnsys}},
url = {https://github.com/pyansys},
year = {2021}
}
@article{Powel1964,
abstract = {A simple variation of the well-known method of minimizing a function of several variables by changing one parameter at a time is described. This variation is such that when the procedure is applied to a quadratic form, it causes conjugate directions to be chosen, so the ultimate rate of convergence is fast when the method is used to minimize a general function. A further variation completes the method, and its ensures that the convergence rate from a bad approximation to a minimum is always efficient. Practical applications of the procedure have proved to be very satisfactory, and numerical examples are given in which functions of up to twenty variables are minimized.},
author = {Powell, M J D},
doi = {10.1093/comjnl/7.2.155},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/powell1964.pdf:pdf},
issn = {0010-4620},
journal = {The Computer Journal},
number = {2},
pages = {155--162},
title = {{An efficient method for finding the minimum of a function of several variables without calculating derivatives}},
url = {https://doi.org/10.1093/comjnl/7.2.155},
volume = {7},
year = {1964}
}
@article{Buitinck2013,
abstract = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
archivePrefix = {arXiv},
arxivId = {1309.0238},
author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"{e}}l},
eprint = {1309.0238},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/1309.0238.pdf:pdf},
journal = {Machine Learning},
pages = {1--15},
title = {{API design for machine learning software: experiences from the scikit-learn project}},
url = {http://arxiv.org/abs/1309.0238},
year = {2013}
}
@article{Pianosi2018,
abstract = {In a previous paper we introduced a distribution-based method for Global Sensitivity Analysis (GSA), called PAWN, which uses cumulative distribution functions of model outputs to assess their sensitivity to the model's uncertain input factors. Over the last three years, PAWN has been employed in the environmental modelling field as a useful alternative or complement to more established variance-based methods. However, a major limitation of PAWN up to now was the need for a tailored sampling strategy to approximate the sensitivity indices. Furthermore, this strategy required three tuning parameters whose optimal choice was rather unclear. In this paper, we present an alternative approximation procedure that tackles both issues and makes PAWN applicable to a generic sample of inputs and outputs while requiring only one tuning parameter. The new implementation therefore allows the user to estimate PAWN indices as complementary metrics in multi-method GSA applications without additional computational cost.},
author = {Pianosi, Francesca and Wagener, Thorsten},
doi = {10.1016/j.envsoft.2018.07.019},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Pianosi, Wagener - 2018 - Distribution-based sensitivity analysis from a generic input-output sample.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Distribution-based methods,Global sensitivity analysis,Moment-independent methods,Multi-method GSA},
number = {August 2018},
pages = {197--207},
publisher = {Elsevier},
title = {{Distribution-based sensitivity analysis from a generic input-output sample}},
url = {https://doi.org/10.1016/j.envsoft.2018.07.019},
volume = {108},
year = {2018}
}
@book{Saltelli2008,
abstract = {Complex mathematical and computational models are used in all areas of society and technology and yet model based science is increasingly contested or refuted, especially when models are applied to controversial themes in domains such as health, the environment or the economy. More stringent standards of proofs are demanded from model-based numbers, especially when these numbers represent potential financial losses, threats to human health or the state of the environment. Quantitative sensitivity analysis is generally agreed to be one such standard. Mathematical models are good at mapping assumptions into inferences. A modeller makes assumptions about laws pertaining to the system, about its status and a plethora of other, often arcane, system variables and internal model settings. To what extent can we rely on the model-based inference when most of these assumptions are fraught with uncertainties? Global Sensitivity Analysis offers an accessible treatment of such problems via quantitative sensitivity analysis, beginning with the first principles and guiding the reader through the full range of recommended practices with a rich set of solved exercises. The text explains the motivation for sensitivity analysis, reviews the required statistical concepts, and provides a guide to potential applications. The book: Provides a self-contained treatment of the subject, allowing readers to learn and practice global sensitivity analysis without further materials. Presents ways to frame the analysis, interpret its results, and avoid potential pitfalls. Features numerous exercises and solved problems to help illustrate the applications. Is authored by leading sensitivity analysis practitioners, combining a range of disciplinary backgrounds. Postgraduate students and practitioners in a wide range of subjects, including statistics, mathematics, engineering, physics, chemistry, environmental sciences, biology, toxicology, actuarial sciences, and econometrics will find much of use here. This book will prove equally valuable to engineers working on risk analysis and to financial analysts concerned with pricing and hedging.},
address = {West Sussex},
author = {Saltelli, Andrea and Andres, Terry and Campolongo, Francesca and Cariboni, Jessica and Gatelli, Debora and Saisana, Michaela and Tarantola, Stefano and Ratto, Marco and Andres, Terry and Campolongo, Francesca and Cariboni, Jessica and Gatelli, Debora and Saisana, Michaela and Tarantola, Stefano},
booktitle = {Global Sensitivity Analysis: The Primer},
doi = {10.1002/9780470725184},
editor = {{John Wiley & Sons Ltd}},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Saltelli et al. - 2008 - Global sensitivity analysis The primer(2).pdf:pdf},
isbn = {9780470725184},
pages = {1--292},
publisher = {John Wiley & Sons Ltd},
title = {{Global sensitivity analysis: The primer}},
year = {2008}
}
@book{Gandomi2013,
abstract = {Differences in incidence of suicide attempts during phases of bipolar I and II disorders With a lifetime risk of a non-fatal suicide attempt ranging from 25% to 56% (1-3), patients with bipolar disorder (BD) are at higher risk for suicide attempts than are patients with any other Axis I disorder (4). While information on risk factors for suicidal behavior is accumulating (5), a major problem for research in this area is the lack of studies relating suicidal behavior to the most pathognomonic feature of the disorder: the recurrent and pleomorphic course. The long-term course of BD is chronic and dominated by depressive symptoms (6-8). In two recent prospective studies, suicidal behavior was related to depressive aspects of the illness (9, 10). Marangell et al. (9) found that history of suicide attempts and percentage of days spent depressed in the year prior to the participants{\~{O}} entry into the Valtonen HM, Suominen K, Haukka J, Mantere O, Leppa¨ma¨kiLeppa¨ma¨Leppa¨ma¨ki S, Arvilommi P, Isometsa¨ETIsometsa¨ET. Differences in incidence of suicide attempts during phases of bipolar I and II disorders. Bipolar Disord 2008: 10: 588-596. {\textordfeminine} 2008 The Authors Journal compilation {\textordfeminine} 2008 Blackwell Munksgaard Background: Differences in the incidence of suicide attempts during various phases of bipolar disorder (BD), or the relative importance of static versus time-varying risk factors for overall risk for suicide attempts, are unknown.},
author = {Gandomi, Amir Hossein and Yang, Xin-She and Talatahari, Siamak and Alavi, Amir Hossein},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Gandomi et al. - 2013 - Metaheuristic applications in structures and infrastructures.pdf:pdf},
isbn = {978-0-12-398364-0},
pages = {577},
publisher = {ELSEVIER},
title = {{Metaheuristic applications in structures and infrastructures}},
year = {2013}
}
@book{Kitanidis1997,
author = {Kitanidis, P. K.},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/P. K. Kitanidis - Introduction to Geostatistics_ Applications in Hydrogeology (1997) - libgen.lc.pdf:pdf},
isbn = {9780521587471},
publisher = {Cambridge University Press, UK},
title = {{Introduction to Geostatistics}},
year = {1997}
}
@article{Brownlee2020a,
abstract = {{\ldots} 1.4 Causes of Class Imbalance {\ldots} 3.5 Compounding Effect of Data Distribution 3.6 Further Reading {\ldots} 211 211 212 215 217 218 220 221 Page 7. CONTENTS vi 19 Cost-Sensitive Deep Learning in Keras 19.1 Tutorial Overview {\ldots}},
author = {Brownlee, Jason},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Brownlee - 2020 - Imbalanced Classification with Python - Choose Better Metrics, Balance Skewed Classes, and Apply Cost-Sensitive Learni.pdf:pdf},
journal = {Machine Learning Mastery},
pages = {463},
title = {{Imbalanced Classification with Python - Choose Better Metrics, Balance Skewed Classes, and Apply Cost-Sensitive Learning}},
url = {https://books.google.com/books?hl=en&lr=&id=jaXJDwAAQBAJ&oi=fnd&pg=PP1&dq=%22intrusion+detection%22%7C%22anomaly+intrusion+detection%22%7C%22network+intrusion+detection%22%7C%22anomaly+based+network+intrusion+detection%22+%22imbalance+problem%22%7C%22imba},
year = {2020}
}
@article{McKay2000a,
abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function. {\textcopyright} 2000 Taylor & Francis Group, LLC.},
author = {McKay, M. D. and Beckman, R. J. and Conover, W. J.},
doi = {10.1080/00401706.2000.10485979},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/McKay, Beckman, Conover - 2000 - A comparison of three methods for selecting values of input variables in the analysis of output from a.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Latin hypercube sampling,Sampling techniques,Simulation techniques,Variance reduction},
number = {1},
pages = {55--61},
title = {{A comparison of three methods for selecting values of input variables in the analysis of output from a computer code}},
volume = {42},
year = {2000}
}
@article{Plischke2010,
abstract = {We present an algorithm named EASI that estimates first order sensitivity indices from given data using Fast Fourier Transformations. Hence it can be used as a post-processing module for pre-computed model evaluations. Ideas for the estimation of higher order sensitivity indices are also discussed. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Plischke, Elmar},
doi = {10.1016/j.ress.2009.11.005},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Plischke - 2010 - An effective algorithm for computing global sensitivity indices (EASI).pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Correlation ratio,FAST method,Global sensitivity analysis,Post-processing algorithm,Sobol' sensitivity indices,Space filling curves},
number = {4},
pages = {354--360},
publisher = {Elsevier},
title = {{An effective algorithm for computing global sensitivity indices (EASI)}},
url = {http://dx.doi.org/10.1016/j.ress.2009.11.005},
volume = {95},
year = {2010}
}
@article{Degeling2020,
abstract = {Metamodels can be used to reduce the computational burden associated with computationally demanding analyses of simulation models, although applications within health economics are still scarce. Besides a lack of awareness of their potential within health economics, the absence of guidance on the conceivably complex and time-consuming process of developing and validating metamodels may contribute to their limited uptake. To address these issues, this article introduces metamodeling to the wider health economic audience and presents a process for applying metamodeling in this context, including suitable methods and directions for their selection and use. General (i.e., non–health economic specific) metamodeling literature, clinical prediction modeling literature, and a previously published literature review were exploited to consolidate a process and to identify candidate metamodeling methods. Methods were considered applicable to health economics if they are able to account for mixed (i.e., continuous and discrete) input parameters and continuous outcomes. Six steps were identified as relevant for applying metamodeling methods within health economics: 1) the identification of a suitable metamodeling technique, 2) simulation of data sets according to a design of experiments, 3) fitting of the metamodel, 4) assessment of metamodel performance, 5) conducting the required analysis using the metamodel, and 6) verification of the results. Different methods are discussed to support each step, including their characteristics, directions for use, key references, and relevant R and Python packages. To address challenges regarding metamodeling methods selection, a first guide was developed toward using metamodels to reduce the computational burden of analyses of health economic models. This guidance may increase applications of metamodeling in health economics, enabling increased use of state-of-the-art analyses (e.g., value of information analysis) with computationally burdensome simulation models.},
author = {Degeling, Koen and IJzerman, Maarten J. and Lavieri, Mariel S. and Strong, Mark and Koffijberg, Hendrik},
doi = {10.1177/0272989X20912233},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Degeling et al. - 2020 - Introduction to Metamodeling for Reducing Computational Burden of Advanced Analyses with Health Economic Models.pdf:pdf},
issn = {1552681X},
journal = {Medical Decision Making},
keywords = {computational burden,emulators,metamodeling,simulation,surrogate models},
number = {3},
pages = {348--363},
pmid = {32428428},
title = {{Introduction to Metamodeling for Reducing Computational Burden of Advanced Analyses with Health Economic Models: A Structured Overview of Metamodeling Methods in a 6-Step Application Process}},
volume = {40},
year = {2020}
}
@article{Tarantola2006,
abstract = {We present two methods for the estimation of main effects in global sensitivity analysis. The methods adopt Satterthwaite's application of random balance designs in regression problems, and extend it to sensitivity analysis of model output for non-linear, non-additive models. Finite as well as infinite ranges for model input factors are allowed. The methods are easier to implement than any other method available for global sensitivity analysis, and reduce significantly the computational cost of the analysis. We test their performance on different test cases, including an international benchmark on safety assessment for nuclear waste disposal originally carried out by OECD/NEA. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Tarantola, S. and Gatelli, D. and Mara, T. A.},
doi = {10.1016/j.ress.2005.06.003},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Tarantola, Gatelli, Mara - 2006 - Random balance designs for the estimation of first order global sensitivity indices.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Computational models,Global sensitivity analysis,Uncertainty analysis},
number = {6},
pages = {717--727},
title = {{Random balance designs for the estimation of first order global sensitivity indices}},
volume = {91},
year = {2006}
}
@article{Shrinkage2016,
author = {Shrinkage, Regression},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Shrinkage - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection via the Lasso}},
url = {jstor.org/stable/2346178},
volume = {58},
year = {1996}
}
@book{Sarmas2020,
author = {Sarmas, Elissaios and Xidonas, Panos and Doukas, Haris},
booktitle = {Springer Optimization and Its Applications},
doi = {10.1007/978-3-030-53743-2_1},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/[Springer Optimization and Its Applications №163] Elissaios Sarmas, Panos Xidonas, Haris Doukas - Multicriteria Portfolio Construction w.pdf:pdf},
isbn = {9783030537425},
issn = {19316836},
pages = {1--176},
title = {{Multicriteria Portfolio Construction with Python}},
volume = {163},
year = {2020}
}
@article{Tissot2012,
abstract = {This paper deals with the random balance design method (RBD) and its hybrid approach, RBD-FAST. Both these global sensitivity analysis methods originate from Fourier amplitude sensitivity test (FAST) and consequently face the main problems inherent to discrete harmonic analysis. We present here a general way to correct a bias which occurs when estimating sensitivity indices (SIs) of any order - except total SI of single factor or group of factors - by the random balance design method (RBD) and its hybrid version, RBD-FAST. In the RBD case, this positive bias has been recently identified in a paper by Xu and Gertner [1]. Following their work, we propose a bias correction method for first-order SIs estimates in RBD. We then extend the correction method to the SIs of any order in RBD-FAST. At last, we suggest an efficient strategy to estimate all the first- and second-order SIs using RBD-FAST. {\textcopyright} 2012 Elsevier Ltd.},
author = {Tissot, Jean Yves and Prieur, Cl{\'{e}}mentine},
doi = {10.1016/j.ress.2012.06.010},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Tissot, Prieur - 2012 - Bias correction for the estimation of sensitivity indices based on random balance designs.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Bias correction,Global sensitivity analysis,RBD-FAST,Random balance design,Sensitivity indices},
pages = {205--213},
title = {{Bias correction for the estimation of sensitivity indices based on random balance designs}},
volume = {107},
year = {2012}
}
@article{Saad2020,
abstract = {Greedy algorithms for feature selection are widely used for recovering sparse high-dimensional vectors in linear models. In classical procedures, the main emphasis was put on the sample complexity, with little or no consideration of the computation resources required. We present a novel online algorithm: Online Orthogonal Matching Pursuit (OOMP) for online support recovery in the random design setting of sparse linear regression. Our procedure selects features sequentially, alternating between allocation of samples only as needed to candidate features, and optimization over the selected set of variables to estimate the regression coefficients. Theoretical guarantees about the output of this algorithm are proven and its computational complexity is analysed.},
archivePrefix = {arXiv},
arxivId = {2011.11117},
author = {Saad, El Mehdi and Blanchard, Gilles and Arlot, Sylvain},
eprint = {2011.11117},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Saad, Blanchard, Arlot - 2020 - Online Orthogonal Matching Pursuit.pdf:pdf},
journal = {arXiv},
number = {Ic},
pages = {1--46},
title = {{Online Orthogonal Matching Pursuit}},
url = {http://arxiv.org/abs/2011.11117},
year = {2020}
}
@article{Iwanaga1815,
abstract = {Sensitivity analysis is now considered a standard practice in environmental modeling. Several open-source libraries, such as the Sensitivity Analysis Library (SALib), have been published in the recent past aimed at simplifying the application of sensitivity analyses. Still, there remain issues in software usability and accessibility, as well as a lack of guidance in the interpretation of sensitivity analysis results. This paper describes the changes made and planned to SALib to advance the ease with which modelers may conduct sensitivity analysis and interpret results. We further offer our perspectives from the past 7 years of maintaining SALib for the consideration of those aspiring to launch their own software for sensitivity analysis, develop methodology, or those otherwise interested in becoming involved in a project like SALib. These include the value of a community of practice to foster best practices for sensitivity analysis, the potential for collaboration across different software (for sensitivity analysis) platforms, and the need to specifically support the software development that underpins computational science. Keywords sensitivity analysis; community of practice; software accessibility Code availability The SALib project is hosted on GitHub (at https://github.com/SALib/SALib) and is made available under the MIT license. Data, code, and figures for citation analysis conducted for this publication are found in Iwanaga (2021) accessible via https://doi.org/10.5281/zenodo.5523624. 1. Introduction The explosive growth and availability of computational power has increased the complexity of environmental modeling analyses, both in terms of the models themselves, as well as the data produced and collected. Sensitivity analysis (SA) is now considered a standard practice in modeling workflows as it can help navigate this complexity. At its core SA illuminates the behavior of a system by exploring and mapping the relationship between the inputs and outputs. Application of SA aids in identification of the relative magnitudes at which model factors drive results (factor prioritization); screening out non-influential parameters to reduce dimensionality of a model (factor fixing); better understanding how the input space relates to an interesting area of the output space (factor mapping); and supports meta-modelling (Saltelli et al., 2008). Application of SA},
author = {Iwanaga, Takuya and Usher, Will and Herman, Jon},
doi = {10.18174/sesmo.18155},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Iwanaga, Usher, Herman - 1815 - Toward SALib 2.0 Advancing the accessibility and interpretability of global sensitivity analyses.pdf:pdf},
journal = {Socio-Environmental Systems Modelling},
keywords = {community of practice,sensitivity analysis,software accessibility},
pages = {2022},
title = {{Toward SALib 2.0: Advancing the accessibility and interpretability of global sensitivity analyses}},
url = {http://www.sesmo.org},
volume = {4},
year = {1815}
}
@techreport{Escribano2022,
address = {Zaragoza},
author = {Escribano, N and Lahuerta, F},
institution = {Itainnova},
title = {{Modelado parametrico micro de materiales (E1.4.1) [2300_I229001]}},
year = {2022}
}
@article{Ruano2012,
abstract = {In this paper, a revised version of the Morris approach, which includes an improved sampling strategy based on trajectory design, has been adapted to the screening of the most influential parameters of a fuzzy controller applied to WWTPs. Due to the high number of parameters, a systematic approach has been proposed to apply this improved sampling strategy with low computational demand. In order to find out the proper repetition number of elementary effects of each input factor on model output (EE i) calculations, an iterative and automatic procedure has been applied. The results show that the sampling strategy has a significant effect on the parameter significance ranking and that random sampling could lead to a non-proper coverage of the parameter space. {\textcopyright} 2012 Elsevier Ltd.},
author = {Ruano, M. V. and Ribes, J. and Seco, A. and Ferrer, J.},
doi = {10.1016/j.envsoft.2012.03.008},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Ruano et al. - 2012 - An improved sampling strategy based on trajectory design for application of the Morris method to systems with many.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Fuzzy controllers,Morris screening,Sampling strategy,Sensitivity analysis},
pages = {103--109},
publisher = {Elsevier Ltd},
title = {{An improved sampling strategy based on trajectory design for application of the Morris method to systems with many input factors}},
url = {http://dx.doi.org/10.1016/j.envsoft.2012.03.008},
volume = {37},
year = {2012}
}
@techreport{Lahuerta2021z,
address = {Zaragoza},
author = {Lahuerta, F},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Lahuerta - 2021 - State of Art of Generative Design methods and tools 2300_I219010.pdf:pdf},
institution = {Itainnova},
title = {{State of Art of Generative Design methods and tools [2300_I219010]}},
year = {2021}
}
@article{Zou2005a,
abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso. {\textcopyright} 2005 Royal Statistical Society.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Zou, Hastie - 2005 - Regularization and variable selection via the elastic net.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection},
number = {2},
pages = {301--320},
title = {{Regularization and variable selection via the elastic net}},
volume = {67},
year = {2005}
}
@article{Rabitz2010,
abstract = {A new unified global sensitivity analysis framework is introduced for systems whose input probability distribution can be independent and/or correlated. For correlated inputs, three sensitivity indices are defined to fully describe the total, structural (reflecting the system structure) and correlative (reflecting the correlated input probability distribution) contributions for an input or a subset of inputs. The magnitudes of all three indices need to be considered in order to quantitatively determine the relative importance of the inputs. For independent inputs, these indices reduce to a single index consistent with previous variance-based methods. This analysis is especially useful for the treatment of laboratory and field data.},
author = {Rabitz, Herschel},
doi = {10.1016/j.sbspro.2010.05.131},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Rabitz - 2010 - Global sensitivity analysis for systems with independent andor correlated inputs.pdf:pdf},
isbn = {1609258096},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Data correlation,Meta-modeling,Sensitivity analysis,Variance analysis},
number = {6},
pages = {7587--7589},
title = {{Global sensitivity analysis for systems with independent and/or correlated inputs}},
volume = {2},
year = {2010}
}
@article{Baroni2020,
abstract = {We present a new strategy for performing global sensitivity analysis capable to estimate main and interaction effects from a generic sampling design. The new strategy is based on a meaningful combination of variance- and distribution-based approaches. The strategy is tested on four analytic functions and on a hydrological model. Results show that the analysis is consistent with the state-of-the-art Saltelli/Jansen formula but to better quantify the interaction effect between the input factors when the output distribution is skewed. Moreover, the estimation of the sensitivity indices is much more robust requiring a smaller number of simulations runs. Specific settings and alternative methods that can be integrated in the new strategy are also discussed. Overall, the strategy is considered as a new simple and effective tool for performing global sensitivity analysis that can be easily integrated in any environmental modelling framework.},
author = {Baroni, Gabriele and Francke, Till},
doi = {10.1016/j.envsoft.2020.104851},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Baroni, Francke - 2020 - An effective strategy for combining variance- and distribution-based global sensitivity analysis.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Distribution,Generic sampling design,Global sensitivity analysis,Variance},
number = {September},
pages = {104851},
publisher = {Elsevier Ltd},
title = {{An effective strategy for combining variance- and distribution-based global sensitivity analysis}},
url = {https://doi.org/10.1016/j.envsoft.2020.104851},
volume = {134},
year = {2020}
}
@article{Lancaster1981,
abstract = {An analysis of moving least squares (m.l.s.) methods for smoothing and interpolating scattered data is presented. In particular, theorems are proved concerning the smoothness of interpolants and the description of m.l.s. processes as projection methods. Some properties of compositions of the m.l.s. projector, with projectors associated with finiteelement schemes, are also considered. The analysis is accompanied by examples of univariate and bivariate problems.},
author = {Lancaster, P. and Salkauskas, K.},
doi = {10.2307/2007507},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Lancaster, Salkauskas - 1981 - Surfaces Generated by Moving Least Squares Methods.pdf:pdf},
issn = {00255718},
journal = {Mathematics of Computation},
number = {155},
pages = {141},
title = {{Surfaces Generated by Moving Least Squares Methods}},
volume = {37},
year = {1981}
}
@book{Eberhard2000,
abstract = {During the lifetime of almost every woman sooner or later a treatment will be necessary for a urogynaecological disorder, such as urinary incontinence, urinary tract infection, vulvo-vaginal or bladder irritability, genital itching, dyspareunia, genital prolapse or pelvic floor disorder. The causes for these common disorders are many. A successfull therapy should take the polyaetiology of urogynaecological disorders into consideration and adapt the different therapeutical possibilities to the specific disease to be treated as well as the patient's needs. The possibilities of conservative treatment are drinking and micturition training, physical therapy with the help of vaginal cones, balls and electromyographic biofeedback devices, oestrogens, pessaries, drug therapy of urinary tract infections or urge incontinence. The use of these different possibilities will be discussed thoroughly. In cases where conservative therapy is insufficient the indication for surgery as well as the timing and choice of the surgical procedure will also be considered.},
author = {Eberhard, J. and Geissbuhler, V.},
booktitle = {Journal fur Urologie und Urogynakologie},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Eberhard, Geissbuhler - 2000 - Konservative und operative therapie bei harninkontinenz, deszensus und urogenital-beschwerden.pdf:pdf},
isbn = {026218253X},
issn = {10236090},
number = {5},
pages = {32--46},
publisher = {MIT},
title = {{Konservative und operative therapie bei harninkontinenz, deszensus und urogenital-beschwerden}},
volume = {7},
year = {2000}
}
@article{McKay2000,
abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function. {\textcopyright} 2000 Taylor & Francis Group, LLC.},
author = {McKay, M. D. and Beckman, R. J. and Conover, W. J.},
doi = {10.2307/1268522},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/McKay, Beckman, Conover - 1979 - A comparison of three methods for selecting values of input variables in the analysis of output from a.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Latin hypercube sampling,Sampling techniques,Simulation techniques,Variance reduction},
number = {2},
pages = {239--245},
title = {{A comparison of three methods for selecting values of input variables in the analysis of output from a computer code}},
volume = {21},
year = {1979}
}
@article{Owen2007,
abstract = {Ridge regression and the lasso are regularized versions of least squares regression using L2 and L1 penalties respectively, on the coefficient vector. To make these regressions more robust we may replace least squares with Huber's criterion which is a hybrid of squared error (for relatively small errors) and absolute error (for relatively large ones). A reversed version of Huber's criterion can be used as a hybrid penalty function. Relatively small coefficients contribute their L1 norm to this penalty while larger ones cause it to grow quadratically. This hybrid sets some coefficients to 0 (as lasso does) while shrinking the larger coefficients the way ridge regression does. Both the Huber and reversed Huber penalty functions employ a scale parameter. We provide an objective function that is jointly convex in the regression coefficient vector and these two scale parameters},
author = {Owen, Art B.},
doi = {10.1090/conm/443/08555},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Owen - 2007 - A robust hybrid of lasso and ridge regression.pdf:pdf},
journal = {researchgate},
number = {October},
pages = {59--71},
title = {{A robust hybrid of lasso and ridge regression}},
year = {2007}
}
@book{Looss2017,
abstract = {Sensitivity analysis provides users of mathematical and simulation models with tools to appreciate the dependency of the model output from model input and to investigate how important is each model input in determining its output. All application areas are concerned, from theoretical physics to engineering and socio-economics. This introductory paper provides the sensitivity analysis aims and objectives in order to explain the composition of the overall "Sensitivity Analysis" chapter of the Springer Handbook. It also describes the basic principles of sensitivity analysis, some classification grids to understand the application ranges of each method, a useful software package, and the notations used in the chapter papers. This section also offers a succinct description of sensitivity auditing, a new discipline that tests the entire inferential chain including model development, implicit assumptions, and normative issues and which is recommended when the inference provided by the model needs to feed into a regulatory or policy process. For the "Sensitivity Analysis" chapter, in addition to this introduction, eight papers have been written by around twenty practitioners from different fields of application. They cover the most widely used methods for this subject: The deterministic methods as the local sensitivity analysis, the experimental design strategies, the sampling-based and variance-based methods developed from the 1980s, and the new importance measures and metamodel-based techniques established and studied since the 2000s. In each paper, toy examples or industrial applications illustrate their relevance and usefulness.},
author = {Looss, Bertrand and Saltelli, Andrea},
booktitle = {Handbook of Uncertainty Quantification},
doi = {10.1007/978-3-319-12385-1_31},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/looss, Saltelli - 2017 - Introduction to sensitivity analysis.pdf:pdf},
isbn = {9783319123851},
keywords = {Computer experiments,Impact assessment,Risk assessment,Sensitivity analysis,Sensitivity auditing,Uncertainty analysis},
pages = {1103--1122},
title = {{Introduction to sensitivity analysis}},
year = {2017}
}
@article{Luo2005,
author = {Luo, Jianwen and Ying, Kui and He, Ping and Bai, Jing},
doi = {10.1016/j.dsp.2004.09.008},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Luo et al. - 2005 - Properties of Savitzky–Golay digital differentiators.pdf:pdf},
issn = {10512004},
journal = {Digital Signal Processing},
keywords = {differentiation,digital differentiator,golay,property,savitzky},
month = {mar},
number = {2},
pages = {122--136},
title = {{Properties of Savitzky–Golay digital differentiators}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1051200404000727},
volume = {15},
year = {2005}
}
@article{Pianosi2015,
abstract = {Variance-based approaches are widely used for Global Sensitivity Analysis (GSA) of environmental models. However, methods that consider the entire Probability Density Function (PDF) of the model output, rather than its variance only, are preferable in cases where variance is not an adequate proxy of uncertainty, e.g. when the output distribution is highly-skewed or when it is multi-modal. Still, the adoption of density-based methods has been limited so far, possibly because they are relatively more difficult to implement. Here we present a novel GSA method, called PAWN, to efficiently compute density-based sensitivity indices. The key idea is to characterise output distributions by their Cumulative Distribution Functions (CDF), which are easier to derive than PDFs. We discuss and demonstrate the advantages of PAWN through applications to numerical and environmental modelling examples. We expect PAWN to increase the application of density-based approaches and to be a complementary approach to variance-based GSA.},
author = {Pianosi, Francesca and Wagener, Thorsten},
doi = {10.1016/j.envsoft.2015.01.004},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Pianosi, Wagener - 2015 - A simple and efficient method for global sensitivity analysis based oncumulative distribution functions.pdf:pdf},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Density-based sensitivity indices,Global sensitivity analysis,Uncertainty analysis,Variance-based sensitivity indices},
pages = {1--11},
publisher = {Elsevier Ltd},
title = {{A simple and efficient method for global sensitivity analysis based oncumulative distribution functions}},
url = {http://dx.doi.org/10.1016/j.envsoft.2015.01.004},
volume = {67},
year = {2015}
}
@article{SOBOL2001271,
abstract = {Global sensitivity indices for rather complex mathematical models can be efficiently computed by Monte Carlo (or quasi-Monte Carlo) methods. These indices are used for estimating the influence of individual variables or groups of variables on the model output.},
annote = {The Second IMACS Seminar on Monte Carlo Methods},
author = {Sobol, I M},
doi = {https://doi.org/10.1016/S0378-4754(00)00270-6},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Sobol′ - 2001 - Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates.pdf:pdf},
issn = {0378-4754},
journal = {Mathematics and Computers in Simulation},
keywords = {Mathematical modelling,Monte Carlo method,Quasi-Monte Carlo method,Sensitivity analysis},
number = {1},
pages = {271--280},
title = {{Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates}},
url = {https://www.sciencedirect.com/science/article/pii/S0378475400002706},
volume = {55},
year = {2001}
}
@article{Cukier1973,
abstract = {A method has been developed to investigate the sensitivity of the solutions of large sets of coupled nonlinear rate equations to uncertainties in the rate coefficients. This method is based on varying all the rate coefficients simultaneously through the introduction of a parameter in such a way that the output concentrations become periodic functions of this parameter at any given time t. The concentrations of the chemical species are then Fourier analyzed at time t. We show via an application of Weyl's ergodic theorem that a subset of the Fourier coefficients is related to 〈∂ci/∂kl〉, the rate of change of the concentration of species i with respect to the rate constant for reaction l averaged over the uncertainties of all the other rate coefficients. Thus a large Fourier coefficient corresponds to a large sensitivity, and a small Fourier coefficient corresponds to a small sensitivity. The amount of numerical integration required to calculate these Fourier coefficients is considerably less than that required in tests of sensitivity where one varies one rate coefficient at a time, while holding all others fixed. The Fourier method developed in this paper is not limited to chemical rate equations, but can be applied to the study of the sensitivity of any large system of coupled, nonlinear differential equations with respect to the uncertainties in the modeling parameters. {\textcopyright} 1973, The American Institute of Physics. All rights reserved.},
author = {Cukier, R. I. and Fortuin, C. M. and Shuler, K. E. and Petschek, A. G. and Schaibly, J. H.},
doi = {10.1063/1.1680571},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Cukier et al. - 1973 - Study of the sensitivity of coupled reaction systems to uncertainties in rate coefficients. I Theory.pdf:pdf},
issn = {10897690},
journal = {Journal of Chemical Physics},
number = {8},
pages = {3873--3878},
title = {{Study of the sensitivity of coupled reaction systems to uncertainties in rate coefficients. I Theory}},
volume = {59},
year = {1973}
}
@article{Scholkopf2013,
abstract = {This book honours the outstanding contributions of Vladimir Vapnik, a rare example of a scientist for whom the following statements hold true simultaneously: his work led to the inception of a new field of research, the theory of statistical learning and empirical inference; he has lived to see the field blossom; and he is still as active as ever. He started analyzing learning algorithms in the 1960s and he invented the first version of the generalized portrait algorithm. He later developed one of the most successful methods in machine learning, the support vector machine (SVM) � more than just an algorithm, this was a new approach to learning problems, pioneering the use of functional analysis and convex optimization in machine learning. Part I of this book contains three chapters describing and witnessing some of Vladimir Vapnik's contributions to science. In the first chapter, L�on Bottou discusses the seminal paper published in 1968 by Vapnik and Chervonenkis that lay the foundations of statistical learning theory, and the second chapter is an English-language translation of that original paper. In the third chapter, Alexey Chervonenkis presents a first-hand account of the early history of SVMs and valuable insights into the first steps in the development of the SVM in the framework of the generalised portrait method. The remaining chapters, by leading scientists in domains such as statistics, theoretical computer science, and mathematics, address substantial topics in the theory and practice of statistical learning theory, including SVMs and other kernel-based methods, boosting, PAC-Bayesian theory, online and transductive learning, loss functions, learnable function classes, notions of complexity for function classes, multitask learning, and hypothesis selection. These contributions include historical and context notes, short surveys, and comments on future research directions. This book will be of interest to researchers, engineers, and graduate students engaged with all aspects of statistical learning.},
author = {Vovk, Vladimir},
doi = {10.1007/978-3-642-41136-6},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Vovk - 2013 - Kernel Ridge Regression.pdf:pdf},
isbn = {9783642411366},
journal = {Empirical Inference: Festschrift in Honor of Vladimir N. Vapnik},
pages = {1--287},
title = {{Kernel Ridge Regression}},
year = {2013}
}
@article{Saltelli1999,
abstract = {A new method for sensitivity analysis (SA) of model output is introduced. It is based on the Fourier amplitude sensitivity test (FAST) and allows the computation of the total contribution of each input factor to the output's variance. The term “total” here means that the factor's main effect, as well as all the interaction terms involving that factor, are included. Although computationally different, the very same measure of sensitivity is offered by the indices of Sobol'. The main advantages of the extended FAST are its robustness, especially at low sample size, and its computational efficiency. The computational aspects of the extended FAST are described. These include (1) the definition of new sets of parametric equations for the search-curve exploring the input space, (2) the selection of frequencies for the parametric equations, and (3) the procedure adopted to estimate the total contributions. We also address the limitations of other global SA methods and suggest that the total-effect indices are ideally suited to perform a global, quantitative, model-independent sensitivity analysis. {\textcopyright} 1999 Taylor & Francis Group, LLC.},
author = {Saltelli, A. and Tarantola, S. and Chan, K. P.S.},
doi = {10.1080/00401706.1999.10485594},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Saltelli, Tarantola, Chan - 1999 - A quantitative model-independent method for global sensitivity analysis of model output.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Computational model,Fourier amplitude sensitivity test (FAST),Nonlinear and nonmonotonic models,Total sensitivity indices},
number = {1},
pages = {39--56},
title = {{A quantitative model-independent method for global sensitivity analysis of model output}},
volume = {41},
year = {1999}
}
@article{Most2008,
abstract = {In real case applications within the virtual prototyping process, it is not always possible to reduce the complexity of the physical models and to obtain numerical models which can be solved quickly. Usually, every single numerical simulation takes hours or even days. Although the progresses in numerical methods and high performance computing, in such cases, it is not possible to explore various model configurations, hence efficient surrogate models are required. Generally the available meta-model techniques show several advantages and disadvan-tages depending on the investigated problem. In this paper we present an automatic ap-proach for the selection of the optimal suitable meta-model for the actual problem. To-gether with an automatic reduction of the variable space using advanced filter techniques an efficient approximation is enabled also for high dimensional problems.},
author = {Most, Thomas and Will, Johannes},
doi = {10.13140/2.1.2194.4007},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Most, Will - 2008 - Metamodel of Optimal Prognosis An Automatic Approach for Variable Reduction and Optimal Metamodel Selection.pdf:pdf},
journal = {Weimarer Optimierungs-und Stochastiktage},
keywords = {meta-modeling,optimal prognosis,regression analysis,surrogate models,variable reduction * Contact},
number = {November 2014},
pages = {1--21},
title = {{Metamodel of Optimal Prognosis: An Automatic Approach for Variable Reduction and Optimal Metamodel Selection}},
year = {2008}
}
@book{Myers2016,
author = {Myers, Raymond H. and Montgomery, Douglas C. and Anderson-Cook, Christine M.},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Myers, Montgomery, Anderson-Cook - 2016 - Response Surface Methodology Process and Product Optimization Using Designed Experiments, 4th.pdf:pdf},
isbn = {978-1-118-91601-8},
publisher = {Wiley},
title = {{Response Surface Methodology: Process and Product Optimization Using Designed Experiments, 4th Edition}},
year = {2016}
}
@article{Operator,
author = {Operator, Selection},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Operator - Unknown - Multi-variate Rregression Metamodel with DOE based on random sampling.pdf:pdf},
number = {l},
pages = {1--24},
title = {{Multi-variate Rregression Metamodel with DOE based on random sampling}},
url = {https://notebooks.githubusercontent.com/view/ipynb?browser=unknown_browser&color_mode=auto&commit=6363d068f3e265ded35c2639a6fbd020fa5df335&device=unknown_device&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f7469727468616a796f}
}
@article{Morris1991,
abstract = {A computational model is a representation of some physical or other system of interest, first expressed mathematically and then implemented in the form of a computer program; it may be viewed as a function of inputs that, when evaluated, produces outputs. Motivation for this article comes from computational models that are deterministic, complicated enough to make classical mathematical analysis impractical and that have a moderate-to-large number of inputs. The problem of designing computational experiments to determine which inputs have important effects on an output is considered. The proposed experimental plans are composed of individually randomized one-factor-at-a-time designs, and data analysis is based on the resulting random sample of observed elementary effects, those changes in an output due solely to changes in a particular input. Advantages of this approach include a lack of reliance on assumptions of relative sparsity of important inputs, monotonicity of outputs with respect to inputs, or adequacy of a low-order polynomial as an approximation to the computational model. {\textcopyright} 1991 American statistical association and the American society for quality control.},
author = {Morris, Max D.},
doi = {10.1080/00401706.1991.10484804},
file = {:C\:/Users/Francisco/Documents/Mendeley Desktop/Morris - 1991 - Factorial sampling plans for preliminary computational experiments.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Computational model,Factor screening,Latin hypercube sampling,Sensitivity analysis},
number = {2},
pages = {161--174},
title = {{Factorial sampling plans for preliminary computational experiments}},
volume = {33},
year = {1991}
}
